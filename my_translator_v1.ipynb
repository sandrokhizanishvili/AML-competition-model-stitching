{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f4e76a",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c464e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059267f6",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed10bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation functions\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Load processed data from .npz file\"\"\"\n",
    "    data = dict(np.load(path, allow_pickle=True))\n",
    "    # data['caption2img'] = data['caption2img'].item()\n",
    "    # data['caption2img_idx'] = data['caption2img_idx'].item()\n",
    "    return data\n",
    "\n",
    "def prepare_train_data(data):\n",
    "    \"\"\"Prepare training data from loaded dict\"\"\"\n",
    "    caption_embd = data['captions/embeddings']\n",
    "    image_embd = data['images/embeddings']\n",
    "    # Map caption embeddings to corresponding image embeddings\n",
    "    label = data['captions/label'] # N x M\n",
    "\n",
    "    # repeat the image embeddings according to the label\n",
    "    label_idx = np.nonzero(label)[1]\n",
    "    print(label_idx.shape)\n",
    "    image_embd = image_embd[label_idx]\n",
    "    assert caption_embd.shape[0] == image_embd.shape[0], \"Mismatch in number of caption and image embeddings\"\n",
    "\n",
    "    X = torch.from_numpy(caption_embd).float()\n",
    "    # Map each caption to its corresponding image embedding\n",
    "    y = torch.from_numpy(image_embd).float()\n",
    "    label = torch.from_numpy(label).bool()\n",
    "\n",
    "    print(f\"Train data: {len(X)} captions, {len(image_embd)} images\")\n",
    "    return X, y, label\n",
    "\n",
    "def generate_submission(sample_ids, translated_embeddings, output_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Generate a submission.csv file from translated embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Generating submission file...\")\n",
    "\n",
    "    if isinstance(translated_embeddings, torch.Tensor):\n",
    "        translated_embeddings = translated_embeddings.cpu().numpy()\n",
    "\n",
    "    # Create a DataFrame with sample_id and embeddings\n",
    "\n",
    "    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': translated_embeddings.tolist()})\n",
    "\n",
    "    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n",
    "    print(f\"✓ Saved submission to {output_file}\")\n",
    "    \n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500004ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training functions\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs, lr, MODEL_PATH):\n",
    "    \"\"\"Train the MLP model\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            loss = F.mse_loss(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = F.mse_loss(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            Path(MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"  ✓ Saved best model (val_loss={val_loss:.6f})\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bbb4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Reciprocal Rank (MRR)\n",
    "    Args:\n",
    "        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "    Returns:\n",
    "        mrr: Mean Reciprocal Rank\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    for i in range(len(gt_indices)):\n",
    "        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n",
    "        if matches.size > 0:\n",
    "            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "\n",
    "def recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n",
    "    \"\"\"Compute Recall@k\n",
    "    Args:\n",
    "        pred_indices: (N, N) array of top indices for N queries\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "        k: number of top predictions to consider\n",
    "    Returns:\n",
    "        recall: Recall@k\n",
    "    \"\"\"\n",
    "    recall = 0\n",
    "    for i in range(len(gt_indices)):\n",
    "        if gt_indices[i] in pred_indices[i, :k]:\n",
    "            recall += 1\n",
    "    recall /= len(gt_indices)\n",
    "    return recall\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n",
    "    Args:\n",
    "        pred_indices: (N, K) array of predicted indices for N queries\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "        k: number of top predictions to consider\n",
    "    Returns:\n",
    "        ndcg: NDCG@k\n",
    "    \"\"\"\n",
    "    ndcg_total = 0.0\n",
    "    for i in range(len(gt_indices)):\n",
    "        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n",
    "        if matches.size > 0:\n",
    "            rank = matches[0] + 1\n",
    "            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n",
    "    return ndcg_total / len(gt_indices)\n",
    "\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n",
    "    \"\"\"Evaluate retrieval performance using cosine similarity\n",
    "    Args:\n",
    "        translated_embd: (N_captions, D) translated caption embeddings\n",
    "        image_embd: (N_images, D) image embeddings\n",
    "        gt_indices: (N_captions,) ground truth image indices for each caption\n",
    "        max_indices: number of top predictions to consider\n",
    "    Returns:\n",
    "        results: dict of evaluation metrics\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute similarity matrix\n",
    "    if isinstance(translated_embd, np.ndarray):\n",
    "        translated_embd = torch.from_numpy(translated_embd).float()\n",
    "    if isinstance(image_embd, np.ndarray):\n",
    "        image_embd = torch.from_numpy(image_embd).float()\n",
    "    \n",
    "    n_queries = translated_embd.shape[0]\n",
    "    device = translated_embd.device\n",
    "    \n",
    "    # Prepare containers for the fragments to be reassembled\n",
    "    all_sorted_indices = []\n",
    "    l2_distances = []\n",
    "    \n",
    "    # Process in batches - the narrow gate approach\n",
    "    for start_idx in range(0, n_queries, batch_size):\n",
    "        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n",
    "        batch_translated = translated_embd[batch_slice]\n",
    "        batch_img_embd = image_embd[batch_slice]\n",
    "        \n",
    "        # Compute similarity only for this batch\n",
    "        batch_similarity = batch_translated @ batch_img_embd.T\n",
    "\n",
    "        # Get top-k predictions for this batch\n",
    "        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n",
    "        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n",
    "\n",
    "        # Compute L2 distance for this batch\n",
    "        batch_gt = gt_indices[batch_slice]\n",
    "        batch_gt_embeddings = image_embd[batch_gt]\n",
    "        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n",
    "        l2_distances.append(batch_l2)\n",
    "    \n",
    "    # Reassemble the fragments\n",
    "    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n",
    "    \n",
    "    # Apply the sacred metrics to the whole\n",
    "    metrics = {\n",
    "        'mrr': mrr,\n",
    "        'ndcg': ndcg,\n",
    "        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n",
    "        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n",
    "        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n",
    "        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n",
    "        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        name: func(sorted_indices, gt_indices)\n",
    "        for name, func in metrics.items()\n",
    "    }\n",
    "    \n",
    "    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n",
    "    results['l2_dist'] = l2_dist\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ca9fae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- (Paste your mrr, recall_at_k, and ndcg functions here) ---\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_retrieval_with_normalization(\n",
    "    translated_embd, \n",
    "    image_embd, \n",
    "    gt_indices, \n",
    "    max_indices=99, \n",
    "    batch_size=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval by normalizing both inputs first,\n",
    "    then comparing batches of queries against the FULL image gallery.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Convert to NumPy for Standardization ---\n",
    "    if isinstance(translated_embd, torch.Tensor):\n",
    "        translated_embd_np = translated_embd.cpu().numpy()\n",
    "    else:\n",
    "        translated_embd_np = translated_embd\n",
    "        \n",
    "    if isinstance(image_embd, torch.Tensor):\n",
    "        image_embd_np = image_embd.cpu().numpy()\n",
    "    else:\n",
    "        image_embd_np = image_embd\n",
    "\n",
    "    # --- 2. NORMALIZATION ADDED ---\n",
    "    # Standardize both predictions and gallery *inside* the function\n",
    "    print(\"Normalizing predictions...\")\n",
    "    scaler_preds = StandardScaler()\n",
    "    translated_embd_norm = scaler_preds.fit_transform(translated_embd_np)\n",
    "    \n",
    "    print(\"Normalizing gallery...\")\n",
    "    scaler_gallery = StandardScaler()\n",
    "    image_embd_norm = scaler_gallery.fit_transform(image_embd_np)\n",
    "    \n",
    "    # --- 3. Convert back to Tensors for GPU processing ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    translated_embd = torch.from_numpy(translated_embd_norm).float().to(device)\n",
    "    image_embd = torch.from_numpy(image_embd_norm).float().to(device)\n",
    "    \n",
    "    # --- 4. Get dimensions and setup gallery ---\n",
    "    n_queries = translated_embd.shape[0]\n",
    "    # Get the (D, N_images) transpose of the FULL gallery\n",
    "    image_gallery_T = image_embd.T  \n",
    "    \n",
    "    all_sorted_indices = []\n",
    "    l2_distances = []\n",
    "    \n",
    "    print(f\"Evaluating {n_queries} queries in batches of {batch_size}...\")\n",
    "    \n",
    "    # --- 5. Process in batches (Corrected) ---\n",
    "    for start_idx in range(0, n_queries, batch_size):\n",
    "        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n",
    "        batch_translated = translated_embd[batch_slice]\n",
    "        \n",
    "        # --- 6. FIX: Compare batch against FULL gallery ---\n",
    "        # (N_batch, D) @ (D, N_images) -> (N_batch, N_images)\n",
    "        batch_similarity = batch_translated @ image_gallery_T\n",
    "\n",
    "        # Get top-k predictions for this batch\n",
    "        # We need to sort and get indices\n",
    "        batch_indices = torch.argsort(batch_similarity, dim=1, descending=True)[:, :max_indices].cpu().numpy()\n",
    "        all_sorted_indices.append(batch_indices)\n",
    "\n",
    "        # --- 7. FIX: Compute L2 distance correctly ---\n",
    "        batch_gt = gt_indices[batch_slice]\n",
    "        # Get embeddings from the FULL normalized gallery\n",
    "        batch_gt_embeddings = image_embd[batch_gt] \n",
    "        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1).cpu()\n",
    "        l2_distances.append(batch_l2)\n",
    "    \n",
    "    # --- 8. Reassemble and call metrics ---\n",
    "    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'mrr': mrr,\n",
    "        'ndcg': lambda p, g: ndcg(p, g, k=max_indices),\n",
    "        'recall_at_1': lambda p, g: recall_at_k(p, g, 1),\n",
    "        'recall_at_3': lambda p, g: recall_at_k(p, g, 3),\n",
    "        'recall_at_5': lambda p, g: recall_at_k(p, g, 5),\n",
    "        'recall_at_10': lambda p, g: recall_at_k(p, g, 10),\n",
    "        'recall_at_50': lambda p, g: recall_at_k(p, g, 50),\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        name: func(sorted_indices, gt_indices)\n",
    "        for name, func in metrics.items()\n",
    "    }\n",
    "    \n",
    "    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n",
    "    results['l2_dist'] = l2_dist\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- HOW TO RUN THE EXPERIMENT ---\n",
    "#\n",
    "# 1. Get your UN-SCALED predictions and UN-SCALED gallery\n",
    "#    pred_embds_val_inverse = sc_y.inverse_transform(pred_embds_val.cpu().numpy())\n",
    "#    y_val_unscaled = valid_data['images/embeddings']\n",
    "#    gt_indices = np.argmax(valid_data['captions/label'], axis=1)\n",
    "#\n",
    "# 2. Call the new function\n",
    "#    results = evaluate_retrieval_with_normalization(\n",
    "#        pred_embds_val_inverse, \n",
    "#        y_val_unscaled, \n",
    "#        gt_indices\n",
    "#    )\n",
    "#\n",
    "# 3. Check the MRR score\n",
    "#    print(results['mrr'])\n",
    "#\n",
    "#    You should see this MRR (e.g., 0.85) is now high again,\n",
    "#    matching your \"scaled vs. scaled\" score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba2473",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb427dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = load_data('data/train/train/train.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03bbcd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000,)\n",
      "Train data: 125000 captions, 125000 images\n"
     ]
    }
   ],
   "source": [
    "# prepare train data\n",
    "X, y, label = prepare_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ecc9634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([125000, 1024]), torch.Size([125000, 1536]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e7cfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([112500, 1024]),\n",
       " torch.Size([12500, 1024]),\n",
       " torch.Size([112500, 1536]),\n",
       " torch.Size([12500, 1536]),\n",
       " torch.Size([112500, 25000]),\n",
       " torch.Size([12500, 25000]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into train and val\n",
    "DATASET_SIZE = len(X)\n",
    "n_train = int(0.9 * len(X))\n",
    "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
    "TRAIN_SPLIT[:n_train] = 1\n",
    "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
    "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
    "labels_train, labels_val = label[TRAIN_SPLIT], label[~TRAIN_SPLIT]\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape, labels_train.shape, labels_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae842923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "\n",
    "# standardize targets\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_val_scaled = scaler_y.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cc9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scalers as a pickle file\n",
    "with open('scaler_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "with open('scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "890fd719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X, y, train_data, label\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e2f810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation label indices\n",
    "img_VAL_SPLIT = labels_val.sum(dim=0) > 0\n",
    "val_label = np.nonzero(labels_val.numpy()[:,img_VAL_SPLIT])[1]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# train label indices\n",
    "img_TRAIN_SPLIT = labels_train.sum(dim=0) > 0\n",
    "train_label = np.nonzero(labels_train.numpy()[:,img_TRAIN_SPLIT])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6683df76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ac1efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save original train/val data and labels\n",
    "# torch.save({'captions/embeddings': X_train, 'images/embeddings': y_train, 'captions/label': labels_train}, 'data/X_y_labels_train.pt')\n",
    "# torch.save({'captions/embeddings': X_val, 'images/embeddings': y_val, 'captions/label': labels_val}, 'data/X_y_labels_val.pt')\n",
    "\n",
    "# save data\n",
    "torch.save({'captions/embeddings': X_train,\n",
    "            'captions/embeddings_standartized': torch.from_numpy(X_train_scaled).float(), \n",
    "            'images/embeddings': y_train,\n",
    "            'images/embeddings_standartized': torch.from_numpy(y_train_scaled).float(),\n",
    "            'captions/label': labels_train,\n",
    "            'captions/label_indices': torch.from_numpy(train_label).long()}, 'data/X_y_labels_train_scaled.pt')\n",
    "torch.save({'captions/embeddings': X_val,\n",
    "            'captions/embeddings_standartized': torch.from_numpy(X_val_scaled).float(),\n",
    "            'images/embeddings': y_val,\n",
    "            'images/embeddings_standartized': torch.from_numpy(y_val_scaled).float(),\n",
    "            'captions/label': labels_val,\n",
    "            'captions/label_indices': torch.from_numpy(val_label).long()}, 'data/X_y_labels_val_scaled.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad91914",
   "metadata": {},
   "source": [
    "# Read Data Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f78620b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scaled data back\n",
    "train = torch.load('data/X_y_labels_train_scaled.pt')\n",
    "val = torch.load('data/X_y_labels_val_scaled.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac6adf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'captions/embeddings': tensor([[-0.7071, -0.0791, -0.6444,  ...,  0.9438, -1.3346,  0.5247],\n",
       "         [ 0.3744, -0.6224, -0.5922,  ...,  0.1355, -1.2186,  0.4079],\n",
       "         [-0.5834, -0.3095, -0.9278,  ..., -0.1768, -0.4095, -0.1322],\n",
       "         ...,\n",
       "         [ 0.4443, -0.4923, -0.5300,  ..., -0.3735,  0.5138, -1.0740],\n",
       "         [ 1.2572, -0.8468, -0.4401,  ...,  0.4742,  0.7457, -0.8515],\n",
       "         [ 0.3378, -0.8111, -1.1533,  ...,  0.7410,  0.4934,  0.0679]]),\n",
       " 'captions/embeddings_standartized': tensor([[-1.0484, -0.2398, -1.1937,  ...,  1.9227, -0.8140,  0.6990],\n",
       "         [ 0.5970, -1.0902, -1.0971,  ...,  0.5235, -0.6814,  0.5296],\n",
       "         [-0.8602, -0.6004, -1.7190,  ..., -0.0170,  0.2435, -0.2536],\n",
       "         ...,\n",
       "         [ 0.7032, -0.8866, -0.9816,  ..., -0.3574,  1.2990, -1.6193],\n",
       "         [ 1.9398, -1.4414, -0.8151,  ...,  1.1098,  1.5640, -1.2967],\n",
       "         [ 0.5412, -1.3856, -2.1370,  ...,  1.5716,  1.2756,  0.0366]]),\n",
       " 'images/embeddings': tensor([[ 0.1224, -0.4013, -0.1244,  ...,  0.2686,  0.4888,  1.1778],\n",
       "         [ 0.1224, -0.4013, -0.1244,  ...,  0.2686,  0.4888,  1.1778],\n",
       "         [ 0.1224, -0.4013, -0.1244,  ...,  0.2686,  0.4888,  1.1778],\n",
       "         ...,\n",
       "         [-0.0528,  0.6580,  0.2983,  ...,  0.2993, -0.4380,  0.6484],\n",
       "         [-0.0528,  0.6580,  0.2983,  ...,  0.2993, -0.4380,  0.6484],\n",
       "         [-0.0528,  0.6580,  0.2983,  ...,  0.2993, -0.4380,  0.6484]]),\n",
       " 'images/embeddings_standartized': tensor([[-1.0878, -1.4801, -0.0469,  ...,  0.5059,  0.0360,  2.7207],\n",
       "         [-1.0878, -1.4801, -0.0469,  ...,  0.5059,  0.0360,  2.7207],\n",
       "         [-1.0878, -1.4801, -0.0469,  ...,  0.5059,  0.0360,  2.7207],\n",
       "         ...,\n",
       "         [-1.5101,  1.4579,  1.0158,  ...,  0.5842, -1.9490,  1.4042],\n",
       "         [-1.5101,  1.4579,  1.0158,  ...,  0.5842, -1.9490,  1.4042],\n",
       "         [-1.5101,  1.4579,  1.0158,  ...,  0.5842, -1.9490,  1.4042]]),\n",
       " 'captions/label': tensor([[ True, False, False,  ..., False, False, False],\n",
       "         [ True, False, False,  ..., False, False, False],\n",
       "         [ True, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]),\n",
       " 'captions/label_indices': tensor([    0,     0,     0,  ..., 22499, 22499, 22499])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "736d67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = train['captions/embeddings_standartized']\n",
    "y_train_scaled = train['images/embeddings_standartized']\n",
    "labels_train = train['captions/label']\n",
    "labels_train_indices = train['captions/label_indices']\n",
    "y_train = train['images/embeddings']\n",
    "\n",
    "\n",
    "X_val_scaled = val['captions/embeddings_standartized']\n",
    "y_val_scaled = val['images/embeddings_standartized']\n",
    "labels_val = val['captions/label']\n",
    "labels_val_indices = val['captions/label_indices']\n",
    "y_val = val['images/embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff962bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, val\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c242d",
   "metadata": {},
   "source": [
    "*Make Padding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e96fccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate padding needed\n",
    "padding_needed = 1536 - 1024  # This is 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "043b5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = F.pad(X_train_scaled, (0, padding_needed))\n",
    "X_val_scaled = F.pad(X_val_scaled, (0, padding_needed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([112500, 1536]), torch.Size([12500, 1536]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, X_val_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ceb613",
   "metadata": {},
   "source": [
    "# SVD approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcf1130f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536, 112500])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57fefd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112500, 1536])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf6697f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SVD (Procrustes) solution...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Load your pre-processed training data ---\n",
    "# (Make sure these are the NumPy arrays, not Tensors)\n",
    "# X_padded shape: (125000, 1536) - Standardized & Padded\n",
    "# Y_scaled shape: (125000, 1536) - Standardized\n",
    "# X_padded = ... \n",
    "# Y_scaled = ...\n",
    "\n",
    "print(\"Calculating SVD (Procrustes) solution...\")\n",
    "\n",
    "# --- 2. Calculate the Covariance Matrix (M) ---\n",
    "# Shape: (1536, 112500) @ (112500, 1536) -> (1536, 1536)\n",
    "M = X_train_scaled.T @ y_train_scaled\n",
    "\n",
    "# --- 3. Perform SVD on M ---\n",
    "# U and V_transpose (Vt) will be the rotation matrices\n",
    "# U shape: (1536, 1536)\n",
    "# Vt shape: (1536, 1536)\n",
    "U, S, Vt = np.linalg.svd(M)\n",
    "\n",
    "# # --- 4. Calculate the optimal Translator Matrix (W) ---\n",
    "# # This is the \"Procrustes\" solution\n",
    "W_translator = U @ Vt\n",
    "\n",
    "# print(\"SVD Translator matrix 'W' calculated.\")\n",
    "\n",
    "# # --- 5. Save your translator ---\n",
    "np.save('svd_translator.npy', W_translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff25ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scaler for features\n",
    "with open('scaler_X.pkl', 'rb') as f:\n",
    "    sc_x = pickle.load(f)\n",
    "\n",
    "# read scaler for targets\n",
    "with open('scaler_Y.pkl', 'rb') as f:\n",
    "    sc_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b70f406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21904\\1136127318.py:17: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  predicted_Y_scaled = test_embds @ W_translator\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21904\\1136127318.py:18: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  predicted_Y_scaled_val = X_val_scaled @ W_translator\n"
     ]
    }
   ],
   "source": [
    "# --- Load your pre-processed test data ---\n",
    "# X_test_padded shape: (N_test, 1536)\n",
    "\n",
    "test_data = load_data(\"data/test/test/test.clean.npz\")\n",
    "\n",
    "test_embds = test_data['captions/embeddings']\n",
    "test_embds = sc_x.transform(test_embds) # Scale the test caption embeddings\n",
    "test_embds = torch.from_numpy(test_embds).float()\n",
    "padding_needed = 1536 - 1024  # This is 512\n",
    "test_embds = F.pad(test_embds, (0, padding_needed)) # make zero padding\n",
    "\n",
    "# --- Load the translator ---\n",
    "W_translator = np.load('svd_translator.npy')\n",
    "\n",
    "# --- Get Predictions ---\n",
    "# (N_test, 1536) @ (1536, 1536) -> (N_test, 1536)\n",
    "predicted_Y_scaled = test_embds @ W_translator\n",
    "predicted_Y_scaled_val = X_val_scaled @ W_translator\n",
    "\n",
    "# --- This is your submission file ---\n",
    "# np.save('submission.npy', predicted_Y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "237a8304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 1536])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_Y_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70c2aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12500, 1536])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_Y_scaled_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b6c818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating submission file...\n",
      "✓ Saved submission to submissions/submission_svd_v1.csv\n"
     ]
    }
   ],
   "source": [
    "submission = generate_submission(test_data['captions/ids'], predicted_Y_scaled, 'submissions/submission_svd_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13904c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_Y_scaled_inverse = sc_y.inverse_transform(predicted_Y_scaled)\n",
    "predicted_Y_scaled_val_inverse = sc_y.inverse_transform(predicted_Y_scaled_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e62dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating submission file...\n",
      "✓ Saved submission to submissions/submission_svd_v1_inverse.csv\n"
     ]
    }
   ],
   "source": [
    "submission = generate_submission(test_data['captions/ids'], predicted_Y_scaled_inverse, 'submissions/submission_svd_v1_inverse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b9b461b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1536)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_Y_scaled_inverse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "802ed351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0075, -0.0022, -0.0025,  ...,  0.0032,  0.0007, -0.0052])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_Y_scaled_val.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8a35a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0144, -0.0144, -0.0144,  ..., -0.0176, -0.0176, -0.0176])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82245e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0039, -0.0039, -0.0039,  ..., -0.0081, -0.0081, -0.0081])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_scaled.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00622e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12500, 1536])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf88ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.8354459077313924),\n",
       " 'ndcg': np.float64(0.8704508432540425),\n",
       " 'recall_at_1': 0.8116,\n",
       " 'recall_at_3': 0.8116,\n",
       " 'recall_at_5': 0.8116,\n",
       " 'recall_at_10': 0.9244,\n",
       " 'recall_at_50': 0.9964,\n",
       " 'l2_dist': 41.15120315551758}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(predicted_Y_scaled_val, y_val, labels_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "487740e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.8375479140185044),\n",
       " 'ndcg': np.float64(0.8721636647915535),\n",
       " 'recall_at_1': 0.81376,\n",
       " 'recall_at_3': 0.81376,\n",
       " 'recall_at_5': 0.81376,\n",
       " 'recall_at_10': 0.92776,\n",
       " 'recall_at_50': 0.99664,\n",
       " 'l2_dist': 50.3546257019043}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(predicted_Y_scaled_val, y_val_scaled, labels_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19c4fde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.7538183872714882),\n",
       " 'ndcg': np.float64(0.8045070140732536),\n",
       " 'recall_at_1': 0.7232,\n",
       " 'recall_at_3': 0.7232,\n",
       " 'recall_at_5': 0.7232,\n",
       " 'recall_at_10': 0.85448,\n",
       " 'recall_at_50': 0.98808,\n",
       " 'l2_dist': 22.219390869140625}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(predicted_Y_scaled_val_inverse, y_val, labels_val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f3c14",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "008f41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"models/mlp_v1.pth\"\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=1536, output_dim=1536, hidden_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # nn.Linear(input_dim, output_dim),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2a6d5981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parameters: 6,295,040\n",
      "\n",
      "3. Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 440/440 [00:35<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.155099, Val Loss = 0.150115\n",
      "  ✓ Saved best model (val_loss=0.150115)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 440/440 [00:49<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.143359, Val Loss = 0.147338\n",
      "  ✓ Saved best model (val_loss=0.147338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 440/440 [00:42<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.139890, Val Loss = 0.145687\n",
      "  ✓ Saved best model (val_loss=0.145687)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 440/440 [00:39<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.137485, Val Loss = 0.145248\n",
      "  ✓ Saved best model (val_loss=0.145248)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 440/440 [00:40<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.135426, Val Loss = 0.144707\n",
      "  ✓ Saved best model (val_loss=0.144707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 440/440 [00:43<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.133618, Val Loss = 0.144191\n",
      "  ✓ Saved best model (val_loss=0.144191)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 440/440 [00:35<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.131914, Val Loss = 0.144025\n",
      "  ✓ Saved best model (val_loss=0.144025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 440/440 [00:32<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.130419, Val Loss = 0.144089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 440/440 [00:32<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.128988, Val Loss = 0.144036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 440/440 [00:31<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.127614, Val Loss = 0.144009\n",
      "  ✓ Saved best model (val_loss=0.144009)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 440/440 [00:31<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.126366, Val Loss = 0.144255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 440/440 [00:31<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.125086, Val Loss = 0.144491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 440/440 [00:38<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.123935, Val Loss = 0.144613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 440/440 [01:18<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.122755, Val Loss = 0.145130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 440/440 [00:32<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 0.121673, Val Loss = 0.145207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 440/440 [00:32<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 0.120605, Val Loss = 0.145296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 440/440 [00:33<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 0.119607, Val Loss = 0.145779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 440/440 [00:34<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 0.118585, Val Loss = 0.146121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 440/440 [00:34<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 0.117617, Val Loss = 0.146234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 440/440 [00:34<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 0.116763, Val Loss = 0.146683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = MLP().to(DEVICE)\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_scaled, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_scaled, y_val), batch_size=BATCH_SIZE)\n",
    "X_train_scaled.shape, X_val_scaled.shape\n",
    "\n",
    "# Train\n",
    "print(\"\\n3. Training...\")\n",
    "model = train_model(model, train_loader, val_loader, DEVICE, EPOCHS, LR, MODEL_PATH)\n",
    "\n",
    "# Load best model for evaluation\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d25026",
   "metadata": {},
   "source": [
    "# Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c6d7f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scaler for features\n",
    "with open('scaler_X.pkl', 'rb') as f:\n",
    "    sc_x = pickle.load(f)\n",
    "\n",
    "# read scaler for targets\n",
    "with open('scaler_Y.pkl', 'rb') as f:\n",
    "    sc_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "701a27ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model for evaluation\n",
    "model = MLP().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c7d3778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data(\"data/test/test/test.clean.npz\")\n",
    "\n",
    "test_embds = test_data['captions/embeddings']\n",
    "test_embds = sc_x.transform(test_embds) # Scale the test caption embeddings\n",
    "test_embds = torch.from_numpy(test_embds).float()\n",
    "padding_needed = 1536 - 1024  # This is 512\n",
    "test_embds = F.pad(test_embds, (0, padding_needed)) # make zero padding\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_embds = model(test_embds.to(DEVICE)).cpu()\n",
    "    pred_embds_val = model(X_val_scaled.to(DEVICE)).cpu()\n",
    "    pred_embds_train = model(X_train_scaled.to(DEVICE)).cpu()\n",
    "\n",
    "# submission = generate_submission(test_data['captions/ids'], pred_embds, 'submissions/submission_v1.csv')\n",
    "# print(f\"Model saved to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d937ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_embds_inversed = torch.from_numpy(sc_y.inverse_transform(pred_embds)).float()\n",
    "pred_embds_val_inversed = torch.from_numpy(sc_y.inverse_transform(pred_embds_val)).float()\n",
    "pred_embds_train_inversed = torch.from_numpy(sc_y.inverse_transform(pred_embds_train)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "71758214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1.6576246548222822e-20)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler().fit_transform(pred_embds).mean()#.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6849f6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8557e-05)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embds.mean()#.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5f7e35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating submission file...\n",
      "✓ Saved submission to submissions/submission_v10.csv\n",
      "Model saved to: models/mlp_v1.pth\n"
     ]
    }
   ],
   "source": [
    "submission = generate_submission(test_data['captions/ids'], StandardScaler().fit_transform(pred_embds), 'submissions/submission_v10.csv')\n",
    "print(f\"Model saved to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fc03c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "40c4b954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.8509497813398018),\n",
       " 'ndcg': np.float64(0.8829141404885074),\n",
       " 'recall_at_1': 0.82856,\n",
       " 'recall_at_3': 0.82856,\n",
       " 'recall_at_5': 0.82856,\n",
       " 'recall_at_10': 0.93672,\n",
       " 'recall_at_50': 0.998,\n",
       " 'l2_dist': 33.63210678100586}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(pred_embds_val, y_val, labels_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "398ff46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.8509497813398018),\n",
       " 'ndcg': np.float64(0.8829141404885074),\n",
       " 'recall_at_1': 0.82856,\n",
       " 'recall_at_3': 0.82856,\n",
       " 'recall_at_5': 0.82856,\n",
       " 'recall_at_10': 0.93672,\n",
       " 'recall_at_50': 0.998,\n",
       " 'l2_dist': 33.63210678100586}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(pred_embds_val, y_val, labels_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.7426362021608336),\n",
       " 'ndcg': np.float64(0.7953180105908544),\n",
       " 'recall_at_1': 0.71144,\n",
       " 'recall_at_3': 0.71144,\n",
       " 'recall_at_5': 0.71144,\n",
       " 'recall_at_10': 0.8436,\n",
       " 'recall_at_50': 0.98496,\n",
       " 'l2_dist': 19.629213333129883}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(pred_embds_val_inversed, y_val, labels_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "78080fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing predictions...\n",
      "Normalizing gallery...\n",
      "Evaluating 12500 queries in batches of 100...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.0003233540050187994),\n",
       " 'ndcg': np.float64(0.0014014752824043528),\n",
       " 'recall_at_1': 8e-05,\n",
       " 'recall_at_3': 0.00016,\n",
       " 'recall_at_5': 0.00016,\n",
       " 'recall_at_10': 0.00048,\n",
       " 'recall_at_50': 0.00352,\n",
       " 'l2_dist': 54.64468002319336}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval_with_normalization(pred_embds_val_inversed, y_val, labels_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8ee355e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.8548669682827283),\n",
       " 'ndcg': np.float64(0.8860412542483317),\n",
       " 'recall_at_1': 0.83288,\n",
       " 'recall_at_3': 0.83288,\n",
       " 'recall_at_5': 0.83288,\n",
       " 'recall_at_10': 0.93976,\n",
       " 'recall_at_50': 0.99808,\n",
       " 'l2_dist': 44.37937545776367}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(pred_embds_val, y_val_scaled, labels_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "13c8ba05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.923201795805102),\n",
       " 'ndcg': np.float64(0.9400569794924678),\n",
       " 'recall_at_1': 0.9103288888888889,\n",
       " 'recall_at_3': 0.9103288888888889,\n",
       " 'recall_at_5': 0.9103288888888889,\n",
       " 'recall_at_10': 0.9775022222222223,\n",
       " 'recall_at_50': 0.9996977777777778,\n",
       " 'l2_dist': 44.5223388671875}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(pred_embds_train, y_train_scaled, labels_train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "539e84b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr': np.float64(0.8400417776251033),\n",
       " 'ndcg': np.float64(0.8736264769635009),\n",
       " 'recall_at_1': 0.8181422222222222,\n",
       " 'recall_at_3': 0.8181422222222222,\n",
       " 'recall_at_5': 0.8181422222222222,\n",
       " 'recall_at_10': 0.9190311111111111,\n",
       " 'recall_at_50': 0.9948977777777778,\n",
       " 'l2_dist': 19.781431198120117}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(pred_embds_train_inversed, y_train, labels_train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "71bc0baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3185,  0.0177,  0.2274,  ..., -0.0558,  0.2325,  0.3599],\n",
       "        [ 0.3185,  0.0177,  0.2274,  ..., -0.0558,  0.2325,  0.3599],\n",
       "        [ 0.3185,  0.0177,  0.2274,  ..., -0.0558,  0.2325,  0.3599],\n",
       "        ...,\n",
       "        [ 0.6575, -0.4332, -0.2424,  ..., -0.4057,  1.0259, -0.5469],\n",
       "        [ 0.6575, -0.4332, -0.2424,  ..., -0.4057,  1.0259, -0.5469],\n",
       "        [ 0.6575, -0.4332, -0.2424,  ..., -0.4057,  1.0259, -0.5469]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c9f5f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31854625,  0.01766749,  0.22744764, ..., -0.05579448,\n",
       "         0.23245949,  0.35991552],\n",
       "       [ 0.31854625,  0.01766749,  0.22744764, ..., -0.05579448,\n",
       "         0.23245949,  0.35991552],\n",
       "       [ 0.31854625,  0.01766749,  0.22744764, ..., -0.05579448,\n",
       "         0.23245949,  0.35991552],\n",
       "       ...,\n",
       "       [ 0.65752709, -0.43324454, -0.24238909, ..., -0.4056725 ,\n",
       "         1.0258552 , -0.54689348],\n",
       "       [ 0.65752709, -0.43324454, -0.24238909, ..., -0.4056725 ,\n",
       "         1.0258552 , -0.54689348],\n",
       "       [ 0.65752709, -0.43324454, -0.24238909, ..., -0.4056725 ,\n",
       "         1.0258552 , -0.54689348]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_y.inverse_transform(y_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1352)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(y_val_scaled, dim=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0bb1f302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5552)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(y_val, dim=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b4beed8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6458)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(pred_embds_val, dim=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7957)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(pred_embds_val_inversed, dim=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "47f21a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1224, -0.4013, -0.1244,  ...,  0.2686,  0.4888,  1.1778],\n",
       "        [ 0.1224, -0.4013, -0.1244,  ...,  0.2686,  0.4888,  1.1778],\n",
       "        [ 0.1224, -0.4013, -0.1244,  ...,  0.2686,  0.4888,  1.1778],\n",
       "        ...,\n",
       "        [-0.0528,  0.6580,  0.2983,  ...,  0.2993, -0.4380,  0.6484],\n",
       "        [-0.0528,  0.6580,  0.2983,  ...,  0.2993, -0.4380,  0.6484],\n",
       "        [-0.0528,  0.6580,  0.2983,  ...,  0.2993, -0.4380,  0.6484]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "53754e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3185,  0.0177,  0.2274,  ..., -0.0558,  0.2325,  0.3599],\n",
       "        [ 0.3185,  0.0177,  0.2274,  ..., -0.0558,  0.2325,  0.3599],\n",
       "        [ 0.3185,  0.0177,  0.2274,  ..., -0.0558,  0.2325,  0.3599],\n",
       "        ...,\n",
       "        [ 0.6575, -0.4332, -0.2424,  ..., -0.4057,  1.0259, -0.5469],\n",
       "        [ 0.6575, -0.4332, -0.2424,  ..., -0.4057,  1.0259, -0.5469],\n",
       "        [ 0.6575, -0.4332, -0.2424,  ..., -0.4057,  1.0259, -0.5469]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0051, -0.0037, -0.0018,  ...,  0.0019, -0.0012, -0.0058])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embds_val.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28fbf415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0039, -0.0039, -0.0039,  ..., -0.0081, -0.0081, -0.0081])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_scaled.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "125f68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0161, -0.0163, -0.0171,  ..., -0.0168, -0.0159, -0.0158])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embds_val_inversed.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b745644a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0144, -0.0144, -0.0144,  ..., -0.0176, -0.0176, -0.0176])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c4527952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embds_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d4c08c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2440e-05)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_scaled.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74131c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c968b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
